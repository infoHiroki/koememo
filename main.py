#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
KoeMemo - éŸ³å£°ãƒ»å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•çš„ã«è­°äº‹éŒ²ã‚’ç”Ÿæˆã™ã‚‹ãƒ„ãƒ¼ãƒ«
ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã¨ã—ã¦å‹•ä½œã—ã€æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã‚’ç›£è¦–ã—ã¦æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å‡¦ç†ã—ã¾ã™ã€‚

ä¸»ãªæ©Ÿèƒ½:
- ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ï¼ˆwatchdogï¼‰
- æ–‡å­—èµ·ã“ã—ï¼ˆfaster-whisperï¼‰
- LLM APIé€£æºï¼ˆOpenAI APIãªã©ï¼‰
- è­°äº‹éŒ²ç”Ÿæˆã¨ä¿å­˜
"""

import os
import sys
import json
import time
import logging
import threading
import subprocess
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import queue
import re
import tkinter as tk
from tkinter import ttk, filedialog, messagebox

# ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼‰
try:
    from faster_whisper import WhisperModel
except ImportError:
    print("ã‚¨ãƒ©ãƒ¼: faster-whisperãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print("pip install faster-whisper ã‚’å®Ÿè¡Œã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

try:
    import requests
except ImportError:
    print("ã‚¨ãƒ©ãƒ¼: requestsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print("pip install requests ã‚’å®Ÿè¡Œã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler, FileCreatedEvent
except ImportError:
    print("ã‚¨ãƒ©ãƒ¼: watchdogãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    print("pip install watchdog ã‚’å®Ÿè¡Œã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

try:
    import tkinter as tk
    from tkinter import ttk, filedialog, messagebox
except ImportError:
    print("ã‚¨ãƒ©ãƒ¼: tkinterãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
    print("Pythonã«tkinterãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',  # nameï¼ˆãƒ­ã‚¬ãƒ¼åï¼‰ã‚’å‰Šé™¤
    datefmt='%Y-%m-%d %H:%M:%S',  # ç§’ã¾ã§ã®è¡¨ç¤ºã«å¤‰æ›´ï¼ˆãƒŸãƒªç§’ã‚’å‰Šé™¤ï¼‰
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("koememo.log", encoding="utf-8")
    ]
)
logger = logging.getLogger("app")

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
CONFIG_PATH = Path(__file__).parent / "config.json"

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
whisper_model = None
file_queue = queue.Queue()
processing_thread = None
observer = None
should_stop = False
config = None  # ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®šå¤‰æ•°


class MediaFileHandler(FileSystemEventHandler):
    """ç›£è¦–ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.supported_extensions = config["file_watcher"]["supported_extensions"]
    
    def on_created(self, event):
        """ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†"""
        if not event.is_directory:
            file_path = event.src_path
            ext = os.path.splitext(file_path)[-1].lower()
            
            if ext in self.supported_extensions:
                logger.info(f"æ–°ã—ã„ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º: {file_path}")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒå¯èƒ½ã«ãªã‚‹ã¾ã§å°‘ã—å¾…æ©Ÿï¼ˆä»–ãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã‚‹æ›¸ãè¾¼ã¿å®Œäº†å¾…ã¡ï¼‰
                time.sleep(1)
                
                # å‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                if is_file_processed(file_path):
                    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™: {file_path}")
                    return
                
                file_queue.put(file_path)


# GUIã‚¯ãƒ©ã‚¹
class KoeMemoGUI:
    """è¨­å®šç”¨GUIã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, root: tk.Tk):
        """ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        self.root = root
        self.root.title("KoeMemo - è¨­å®š")
        
        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ç”»é¢ä¸­å¤®ã«é…ç½®
        self.center_window()
        
        # ã‚¢ã‚¤ã‚³ãƒ³ã®è¨­å®šï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
        icon_path = Path(__file__).parent / "icon.ico"
        if icon_path.exists():
            self.root.iconbitmap(str(icon_path))
        
        # è¨­å®šã®èª­ã¿è¾¼ã¿
        self.config = self.load_config()
        
        # ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹
        self.service_running = False
        self.service_thread = None
        
        # UIæ§‹ç¯‰
        self.build_ui()

    def center_window(self):
        """ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ç”»é¢ä¸­å¤®ã«é…ç½®"""
        screen_width = self.root.winfo_screenwidth()
        screen_height = self.root.winfo_screenheight()
        
        window_width = 900
        window_height = 700
        
        x = (screen_width - window_width) // 2
        y = (screen_height - window_height) // 2
        
        self.root.geometry(f"{window_width}x{window_height}+{x}+{y}")

    def load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                config = json.load(f)
                
                # LLMãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                if "llm_models" not in config:
                    config["llm_models"] = {
                        "openai": [
                            "gpt-4o",
                            "gpt-4o-mini",
                            "gpt-4-turbo",
                            "gpt-4",
                            "gpt-3.5-turbo",
                            "gpt-3.5-turbo-16k"
                        ],
                        "anthropic": [
                            "claude-3-opus-20240229",
                            "claude-3-sonnet-20240229",
                            "claude-3-haiku-20240307"
                        ],
                        "google": [
                            "gemini-1.5-pro",
                            "gemini-1.5-flash",
                            "gemini-pro"
                        ]
                    }
                    # æ›´æ–°ã—ãŸè¨­å®šã‚’ä¿å­˜
                    with open(CONFIG_PATH, "w", encoding="utf-8") as f_save:
                        json.dump(config, f_save, ensure_ascii=False, indent=4)
                    
                return config
        except (FileNotFoundError, json.JSONDecodeError) as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return {}

    def save_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜"""
        try:
            with open(CONFIG_PATH, "w", encoding="utf-8") as f:
                json.dump(self.config, f, ensure_ascii=False, indent=4)
            logger.info("è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def build_ui(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ã®æ§‹ç¯‰"""
        # ã‚¿ãƒ–ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«
        notebook = ttk.Notebook(self.root)
        notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # åŸºæœ¬è¨­å®šã‚¿ãƒ–
        basic_frame = ttk.Frame(notebook, padding=10)
        notebook.add(basic_frame, text="åŸºæœ¬è¨­å®š")
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚¿ãƒ–
        model_frame = ttk.Frame(notebook, padding=10)
        notebook.add(model_frame, text="ãƒ¢ãƒ‡ãƒ«è¨­å®š")
        
        # LLMè¨­å®šã‚¿ãƒ–
        llm_frame = ttk.Frame(notebook, padding=10)
        notebook.add(llm_frame, text="LLMè¨­å®š")
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¿ãƒ–
        template_frame = ttk.Frame(notebook, padding=10)
        notebook.add(template_frame, text="ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
        
        # ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚¿ãƒ–
        models_frame = ttk.Frame(notebook, padding=10)
        notebook.add(models_frame, text="ãƒ¢ãƒ‡ãƒ«ç®¡ç†")
        
        # åŸºæœ¬è¨­å®šã‚¿ãƒ–ã®å†…å®¹
        self.build_basic_settings(basic_frame)
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚¿ãƒ–ã®å†…å®¹
        self.build_model_settings(model_frame)
        
        # LLMè¨­å®šã‚¿ãƒ–ã®å†…å®¹
        self.build_llm_settings(llm_frame)
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¿ãƒ–ã®å†…å®¹
        self.build_template_settings(template_frame)
        
        # ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚¿ãƒ–ã®å†…å®¹
        self.build_models_management(models_frame)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¨æ“ä½œãƒœã‚¿ãƒ³
        status_frame = ttk.Frame(self.root, padding=10)
        status_frame.pack(fill=tk.X, side=tk.BOTTOM)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ©ãƒ™ãƒ«
        self.status_var = tk.StringVar(value="ã‚µãƒ¼ãƒ“ã‚¹ã¯åœæ­¢ã—ã¦ã„ã¾ã™")
        status_label = ttk.Label(status_frame, textvariable=self.status_var)
        status_label.pack(side=tk.LEFT, padx=5)
        
        # ãƒœã‚¿ãƒ³
        self.start_stop_button = ttk.Button(status_frame, text="ã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹", command=self.toggle_service)
        self.start_stop_button.pack(side=tk.RIGHT, padx=5)
        
        ttk.Button(status_frame, text="è¨­å®šä¿å­˜", command=self.save_config_and_reload).pack(side=tk.RIGHT, padx=5)


def get_file_hash(file_path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‘ã‚¹ã®çµ„ã¿åˆã‚ã›ï¼‰"""
    # é«˜é€ŸåŒ–ã®ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‘ã‚¹ã®çµ„ã¿åˆã‚ã›ã‚’ãƒãƒƒã‚·ãƒ¥
    try:
        file_size = os.path.getsize(file_path)
        hash_input = f"{file_path}:{file_size}"
        return hashlib.md5(hash_input.encode()).hexdigest()
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‘ã‚¹ã®ã¿ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
        return hashlib.md5(file_path.encode()).hexdigest()


def is_file_processed(file_path: str) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯"""
    global config
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
    file_hash = get_file_hash(file_path)
    
    # å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
    processed_files = config.get("processed_files", {})
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯ãƒãƒƒã‚·ãƒ¥ãŒä¸€è‡´ã™ã‚‹å ´åˆã¯å‡¦ç†æ¸ˆã¿
    return file_path in processed_files or file_hash in processed_files


def mark_file_as_processed(file_path: str, output_file: str):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
    global config
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
    file_hash = get_file_hash(file_path)
    
    # å‡¦ç†æƒ…å ±ã‚’è¨˜éŒ²
    processed_info = {
        "processed_at": datetime.now().isoformat(),
        "output_file": output_file
    }
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½åŠ 
    if "processed_files" not in config:
        config["processed_files"] = {}
    
    # ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ã‚­ãƒ¼ã¨ã—ã¦ä¿å­˜
    config["processed_files"][file_hash] = processed_info
    
    # è¨­å®šã‚’ä¿å­˜
    save_config(config)
    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†æ¸ˆã¿ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã—ãŸ: {file_path}")
    
    # GUIå®Ÿè¡Œä¸­ã®å ´åˆã¯å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’æ›´æ–°
    try:
        # tkãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¦ã€GUIã®ãƒ«ãƒ¼ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿
        if 'Tk' in sys.modules and hasattr(sys.modules['tkinter'], '_default_root') and sys.modules['tkinter']._default_root:
            root = sys.modules['tkinter']._default_root
            # GUIã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ç¢ºèª
            for widget in root.winfo_children():
                if hasattr(widget, 'master') and hasattr(widget.master, 'update_processed_files'):
                    # é…å»¶å®Ÿè¡Œã§å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’æ›´æ–°
                    root.after(1000, widget.master.update_processed_files)
                    break
    except Exception as e:
        logger.debug(f"GUIã®å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆæ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆç„¡è¦–å¯èƒ½ï¼‰: {e}")


def load_config() -> Dict[str, Any]:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
    try:
        with open(CONFIG_PATH, "r", encoding="utf-8") as f:
            config_data = json.load(f)
            
            # å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒãªã‘ã‚Œã°è¿½åŠ 
            if "processed_files" not in config_data:
                config_data["processed_files"] = {}
            
            return config_data
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)


def save_config(config: Dict[str, Any]):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜"""
    with open(CONFIG_PATH, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=4)


def clean_old_processed_files(max_entries: int = 1000):
    """å¤ã„å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    global config
    
    processed_files = config.get("processed_files", {})
    if not processed_files:
        return
    
    # ã‚¨ãƒ³ãƒˆãƒªæ•°ãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if len(processed_files) > max_entries:
        logger.info(f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ (ç¾åœ¨: {len(processed_files)} ã‚¨ãƒ³ãƒˆãƒª)")
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        sorted_entries = sorted(
            processed_files.items(),
            key=lambda x: x[1].get("processed_at", ""),
            reverse=True
        )
        
        # ä¸Šé™æ•°ã¾ã§å‰Šæ¸›
        config["processed_files"] = dict(sorted_entries[:max_entries])
        save_config(config)
        logger.info(f"å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ {len(processed_files)} ã‹ã‚‰ {len(config['processed_files'])} ã‚¨ãƒ³ãƒˆãƒªã«å‰Šæ¸›ã—ã¾ã—ãŸ")


def load_whisper_model(config: Dict[str, Any]) -> Optional[WhisperModel]:
    """WhisperModelã‚’ãƒ­ãƒ¼ãƒ‰"""
    try:
        model_config = config["transcription"]
        model_size = model_config["model_size"]
        compute_type = model_config["compute_type"]
        
        logger.info(f"ãƒ¢ãƒ‡ãƒ« '{model_size}' ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
        
        # CUDAãŒåˆ©ç”¨å¯èƒ½ãªã‚‰GPUã‚’ä½¿ç”¨
        device = "cuda" if is_cuda_available() else "cpu"
        
        # CPUã§float16ã‚’æŒ‡å®šã•ã‚ŒãŸå ´åˆã¯int8ã«è‡ªå‹•å¤‰æ›
        if device == "cpu" and compute_type == "float16":
            compute_type = "int8"
            logger.info("CPUã§ã®å®Ÿè¡Œã®ãŸã‚ã€è¨ˆç®—ã‚¿ã‚¤ãƒ—ã‚’int8ã«è‡ªå‹•å¤‰æ›´ã—ã¾ã—ãŸã€‚")
        
        model = WhisperModel(
            model_size_or_path=model_size,
            device=device,
            compute_type=compute_type
        )
        
        logger.info(f"ãƒ¢ãƒ‡ãƒ« '{model_size}' ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚({device}ã€{compute_type})")
        return model
    
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def is_cuda_available() -> bool:
    """CUDAãŒåˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹ã‚’ç¢ºèª"""
    try:
        import torch
        return torch.cuda.is_available()
    except ImportError:
        return False


def check_ffmpeg() -> bool:
    """FFmpegãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    try:
        result = subprocess.run(["ffmpeg", "-version"], capture_output=True, text=True)
        return result.returncode == 0
    except FileNotFoundError:
        return False


def transcribe_file(file_path: str, config: Dict[str, Any]) -> Optional[str]:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—èµ·ã“ã—å‡¦ç†"""
    global whisper_model
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ãŒæœªãƒ­ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ­ãƒ¼ãƒ‰
        if whisper_model is None:
            whisper_model = load_whisper_model(config)
            if whisper_model is None:
                return None
        
        # è¨€èªè¨­å®š
        language = config["transcription"]["language"]
        if language == "auto":
            language = None  # Whisperã®è‡ªå‹•æ¤œå‡ºã‚’ä½¿ç”¨
            logger.info("è¨€èªã¯è‡ªå‹•æ¤œå‡ºã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        else:
            logger.info(f"è¨€èªè¨­å®š: {language}")
        
        base_filename = os.path.basename(file_path)
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
        model_size = config["transcription"]["model_size"]
        compute_type = config["transcription"]["compute_type"]
        device = "cuda" if is_cuda_available() else "cpu"
        
        logger.info(f"æ–‡å­—èµ·ã“ã—é–‹å§‹: {base_filename} (ãƒ¢ãƒ‡ãƒ«: {model_size}, ãƒ‡ãƒã‚¤ã‚¹: {device}, è¨ˆç®—ã‚¿ã‚¤ãƒ—: {compute_type})")
        
        # æ–‡å­—èµ·ã“ã—ã®å®Ÿè¡Œ
        segments, info = whisper_model.transcribe(
            file_path,
            language=language,
            beam_size=5,
            task="transcribe"
        )
        
        # çµæœã‚’æ–‡å­—åˆ—ã«ã¾ã¨ã‚ã‚‹ - ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾å‡¦ç†
        result = []
        segment_count = 0
        last_progress_time = time.time()
        estimated_segments = 0  # æ¨å®šã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ï¼ˆéŸ³å£°ã®é•·ã•ã‹ã‚‰æ¦‚ç®—ï¼‰
        
        # segmentsã¯ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãªã®ã§ãƒªã‚¹ãƒˆã«å¤‰æ›ã›ãšã«å‡¦ç†
        for segment in segments:
            segment_count += 1
            
            # 10ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã”ã¨ã€ã¾ãŸã¯5ç§’ã”ã¨ã«é€²æ—ã‚’ãƒ­ã‚°ã«è¡¨ç¤º
            current_time = time.time()
            if segment_count % 10 == 0 or segment_count == 1 or (current_time - last_progress_time) >= 5:
                last_progress_time = current_time
                text_preview = segment.text.strip()
                # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ­ã‚°ã«è¡¨ç¤ºï¼ˆé•·ã„å ´åˆã¯çœç•¥ï¼‰
                if len(text_preview) > 30:
                    text_preview = text_preview[:27] + "..."
                logger.info(f"æ–‡å­—èµ·ã“ã—é€²æ—: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {segment_count} - \"{text_preview}\"")
                
            if should_stop:
                logger.info("æ–‡å­—èµ·ã“ã—å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
                return None
            
            start_time = format_time(segment.start)
            end_time = format_time(segment.end)
            text = segment.text.strip()
            
            if text:
                result.append(f"[{start_time} -> {end_time}] {text}")
        
        logger.info(f"âœ… æ–‡å­—èµ·ã“ã—å®Œäº†: {os.path.basename(file_path)} - åˆè¨ˆ {segment_count} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå‡¦ç†")
        
        return "\n".join(result)
    
    except FileNotFoundError:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return None
    except PermissionError:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“: {file_path}")
        return None
    except RuntimeError as e:
        logger.error(f"Whisperãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return None
    except Exception as e:
        logger.error(f"æ–‡å­—èµ·ã“ã—å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def format_time(seconds: float) -> str:
    """ç§’æ•°ã‚’[HH:MM:SS]å½¢å¼ã«å¤‰æ›ï¼ˆå°æ•°ç‚¹ä»¥ä¸‹ã‚’çœç•¥ï¼‰"""
    hours, remainder = divmod(seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    return f"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}"


def split_transcription(transcription: str, chunk_size: int = 5000) -> List[Dict[str, Any]]:
    """æ–‡å­—èµ·ã“ã—ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
    
    Args:
        transcription: æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆ
        chunk_size: å„ãƒãƒ£ãƒ³ã‚¯ã®æœ€å¤§æ–‡å­—æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5000æ–‡å­—ï¼‰
        
    Returns:
        åˆ†å‰²ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆã€‚å„ãƒãƒ£ãƒ³ã‚¯ã¯è¾æ›¸å½¢å¼ã§ã€
        index, start_time, end_time, contentã‚­ãƒ¼ã‚’æŒã¤
    """
    lines = transcription.split("\n")
    chunks = []
    current_chunk = []
    current_size = 0
    chunk_index = 1
    
    # æ™‚é–“æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã®æ­£è¦è¡¨ç¾
    time_pattern = r"\[(\d{2}:\d{2}:\d{2})"
    
    start_time = None
    end_time = None
    
    for line in lines:
        line_size = len(line) + 1  # æ”¹è¡Œæ–‡å­—ã‚’è€ƒæ…®
        
        # æœ€åˆã®è¡Œã‹ã‚‰é–‹å§‹æ™‚é–“ã‚’æŠ½å‡º
        if not start_time and line:
            start_match = re.search(time_pattern, line)
            if start_match:
                start_time = start_match.group(1)
        
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’è¶…ãˆã‚‹å ´åˆã€æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é–‹å§‹
        if current_size + line_size > chunk_size and current_chunk:
            # æœ€å¾Œã®è¡Œã‹ã‚‰çµ‚äº†æ™‚é–“ã‚’æŠ½å‡º
            if current_chunk[-1]:
                end_match = re.search(time_pattern, current_chunk[-1])
                if end_match:
                    end_time = end_match.group(1)
            
            # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’è¿½åŠ 
            chunks.append({
                "index": chunk_index,
                "start_time": start_time or "00:00:00.000",
                "end_time": end_time or "unknown",
                "content": "\n".join(current_chunk)
            })
            
            # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã®æº–å‚™
            current_chunk = []
            current_size = 0
            chunk_index += 1
            # æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã®é–‹å§‹æ™‚é–“ã¯ã€ç¾åœ¨ã®çµ‚äº†æ™‚é–“ã‹ã‚‰å§‹ã¾ã‚‹
            start_time = end_time
            end_time = None
        
        current_chunk.append(line)
        current_size += line_size
    
    # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
    if current_chunk:
        if current_chunk[-1]:
            end_match = re.search(time_pattern, current_chunk[-1])
            if end_match:
                end_time = end_match.group(1)
        
        chunks.append({
            "index": chunk_index,
            "start_time": start_time or "00:00:00.000",
            "end_time": end_time or "unknown",
            "content": "\n".join(current_chunk)
        })
    
    # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
    for chunk in chunks:
        start_time = chunk['start_time'] if chunk['start_time'] != "unknown" else "00:00:00"
        end_time = chunk['end_time'] if chunk['end_time'] != "unknown" else "æœ€å¾Œã¾ã§"
        logger.info(f"ãƒãƒ£ãƒ³ã‚¯ {chunk['index']}: {start_time} -> {end_time}, ã‚µã‚¤ã‚º: {len(chunk['content'])}æ–‡å­—")
    
    return chunks


def is_long_transcription(transcription: str, config: Dict[str, Any]) -> bool:
    """æ–‡å­—èµ·ã“ã—ãŒé•·ã„ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹
    
    Args:
        transcription: åˆ¤å®šã™ã‚‹æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆ
        config: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
        
    Returns:
        True: é•·ã„æ–‡å­—èµ·ã“ã—ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆ
        False: é€šå¸¸ã®é•·ã•ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆ
    """
    # æ–‡å­—æ•°ã§åˆ¤å®š
    char_count = len(transcription)
    processing_config = config.get("processing", {})
    
    # chunking ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ç¢ºèª
    chunking_enabled = processing_config.get("enable_chunking", True)
    if not chunking_enabled:
        logger.info("åˆ†å‰²å‡¦ç†ãŒç„¡åŠ¹ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚æ¨™æº–å‡¦ç†ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return False
    
    # é¸æŠã•ã‚ŒãŸLLMãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’å–å¾—
    llm_config = config.get("llm", {})
    model_name = llm_config.get("model", "").lower()
    api_type = llm_config.get("api_type", "openai").lower()
    
    # ãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã„ã¦é–¾å€¤ã‚’èª¿æ•´
    chunk_size = processing_config.get("chunk_size", 12000)
    threshold_multiplier = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é–¾å€¤ä¹—æ•°
    
    # ãƒ¢ãƒ‡ãƒ«ç‰¹æ€§ã«åŸºã¥ãèª¿æ•´
    if api_type == "openai":
        if "gpt-4o" in model_name or "gpt-4-turbo" in model_name:
            # GPT-4oã‚„GPT-4-turboã¯ã‚ˆã‚Šå¤§ããªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†å¯èƒ½
            threshold_multiplier = 3
        elif "gpt-3.5" in model_name:
            # GPT-3.5ã¯å°ã•ã‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            threshold_multiplier = 2.5
    elif api_type == "anthropic":
        if "opus" in model_name:
            threshold_multiplier = 3.5
        elif "sonnet" in model_name:
            threshold_multiplier = 3
        else:  # haikuç­‰
            threshold_multiplier = 2.5
    elif api_type == "google":
        if "1.5-pro" in model_name:
            threshold_multiplier = 3.5
        else:
            threshold_multiplier = 3
    
    # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¨ä¹—æ•°ã«åŸºã¥ã„ã¦é–¾å€¤ã‚’è¨ˆç®—
    threshold = int(chunk_size * threshold_multiplier)
    is_long = char_count > threshold
    
    if is_long:
        logger.info(f"é•·ã„æ–‡å­—èµ·ã“ã—ã‚’æ¤œå‡º: ç´„{char_count}æ–‡å­—ï¼ˆé–¾å€¤: {threshold}æ–‡å­—ã€ãƒ¢ãƒ‡ãƒ«: {model_name}ï¼‰")
    
    return is_long


def call_llm_api(transcription: str, config: Dict[str, Any]) -> Optional[str]:
    """LLM APIã‚’å‘¼ã³å‡ºã—ã¦è­°äº‹éŒ²ã‚’ç”Ÿæˆ"""
    try:
        # é•·ã„æ–‡å­—èµ·ã“ã—ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
        if is_long_transcription(transcription, config):
            logger.info("é•·ã„æ–‡å­—èµ·ã“ã—ã‚’æ¤œå‡ºã—ãŸãŸã‚ã€åˆ†å‰²å‡¦ç†ã‚’é©ç”¨ã—ã¾ã™")
            return process_chunked_transcription(transcription, config)
            
        # é€šå¸¸ã®å‡¦ç†ï¼ˆçŸ­ã„æ–‡å­—èµ·ã“ã—ï¼‰
        llm_config = config["llm"]
        api_type = llm_config["api_type"]
        
        # é¸æŠã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ï¼ˆè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        template_name = llm_config.get("selected_template", "default")
        if not template_name or template_name not in config["prompt_templates"]:
            template_name = "default"
            logger.warning(f"æŒ‡å®šã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ '{template_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        
        template = config["prompt_templates"][template_name]
        prompt = template.replace("{transcription}", transcription)
        
        logger.info(f"LLM API ({api_type}) å‘¼ã³å‡ºã—é–‹å§‹ - ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_name}")
        
        result = None
        if api_type == "openai":
            result = call_openai_api(prompt, llm_config)
        elif api_type == "anthropic":
            result = call_anthropic_api(prompt, llm_config)
        elif api_type == "google":
            result = call_google_api(prompt, llm_config)
        else:
            logger.error(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„APIç¨®é¡: {api_type}")
            return None
            
        if result:
            logger.info(f"âœ… LLM API ({api_type}) å‘¼ã³å‡ºã—å®Œäº†: ç´„{len(result)}æ–‡å­—ã®å¿œç­”ã‚’å—ä¿¡")
        else:
            logger.error(f"âŒ LLM API ({api_type}) å‘¼ã³å‡ºã—å¤±æ•—: å¿œç­”ãªã—")
            
        return result
    
    except Exception as e:
        logger.error(f"âŒ LLM APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def call_llm_api_for_chunk(chunk: Dict[str, Any], config: Dict[str, Any]) -> Optional[str]:
    """ãƒãƒ£ãƒ³ã‚¯ç”¨ã®LLM APIå‘¼ã³å‡ºã—
    
    Args:
        chunk: å‡¦ç†ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ï¼ˆindex, start_time, end_time, contentï¼‰
        config: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
        
    Returns:
        è¦ç´„çµæœãƒ†ã‚­ã‚¹ãƒˆã€ã¾ãŸã¯å¤±æ•—æ™‚ã¯None
    """
    try:
        llm_config = config["llm"]
        api_type = llm_config["api_type"]
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—
        template_name = llm_config.get("selected_template", "default")
        if not template_name or template_name not in config["prompt_templates"]:
            template_name = "default"
            logger.warning(f"æŒ‡å®šã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ '{template_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        
        template = config["prompt_templates"][template_name]
        
        # ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’çµ„ã¿è¾¼ã‚“ã ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—éƒ¨åˆ†ã®è¡¨ç¤ºã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã®å‡¦ç†ï¼‰
        start_time = chunk['start_time'] if chunk['start_time'] != "unknown" else "00:00:00"
        end_time = chunk['end_time'] if chunk['end_time'] != "unknown" else "æœ€å¾Œã¾ã§"
        part_info = f"ä¼šè­°è¨˜éŒ² ç¬¬{chunk['index']}éƒ¨ï¼ˆ{start_time}ï½{end_time}ï¼‰"
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’è¿½åŠ 
        modified_template = f"{template}\n\næ³¨: ã“ã‚Œã¯{part_info}ã®è¦ç´„ã§ã™ã€‚"
        prompt = modified_template.replace("{transcription}", chunk["content"])
        
        logger.info(f"LLM API ({api_type}) å‘¼ã³å‡ºã—é–‹å§‹ - ãƒãƒ£ãƒ³ã‚¯ {chunk['index']}")
        
        # é€šå¸¸ã®LLM APIå‘¼ã³å‡ºã—å‡¦ç†ã‚’å®Ÿè¡Œ
        result = None
        if api_type == "openai":
            result = call_openai_api(prompt, llm_config)
        elif api_type == "anthropic":
            result = call_anthropic_api(prompt, llm_config)
        elif api_type == "google":
            result = call_google_api(prompt, llm_config)
        else:
            logger.error(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„APIç¨®é¡: {api_type}")
            return None
        
        if result:
            logger.info(f"âœ… ãƒãƒ£ãƒ³ã‚¯ {chunk['index']} ã®è¦ç´„å®Œäº†: ç´„{len(result)}æ–‡å­—")
            # è¦ç´„ã«ãƒ‘ãƒ¼ãƒˆæƒ…å ±ã‚’è¿½åŠ 
            result = f"## {part_info}\n\n{result}"
        else:
            logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ {chunk['index']} ã®è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        return result
    
    except Exception as e:
        logger.error(f"âŒ ãƒãƒ£ãƒ³ã‚¯ {chunk['index']} ã®LLM APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def process_chunked_transcription(transcription: str, config: Dict[str, Any]) -> Optional[str]:
    """é•·ã„æ–‡å­—èµ·ã“ã—ã®åˆ†å‰²å‡¦ç†
    
    Args:
        transcription: æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“
        config: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
        
    Returns:
        å‡¦ç†çµæœã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã€ã¾ãŸã¯å¤±æ•—æ™‚ã¯None
    """
    # è¨­å®šã‹ã‚‰åˆ†å‰²ã‚µã‚¤ã‚ºã‚’å–å¾—
    processing_config = config.get("processing", {})
    chunk_size = processing_config.get("chunk_size", 5000)
    
    # æ–‡å­—èµ·ã“ã—ã‚’åˆ†å‰²
    chunks = split_transcription(transcription, chunk_size)
    logger.info(f"æ–‡å­—èµ·ã“ã—ã‚’ {len(chunks)} ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")
    
    # å„ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
    chunk_summaries = []
    for chunk in chunks:
        summary = call_llm_api_for_chunk(chunk, config)
        if summary:
            chunk_summaries.append(summary)
        else:
            logger.warning(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ {chunk['index']} ã®è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    # ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯ãŒå‡¦ç†å¤±æ•—ã—ãŸå ´åˆ
    if not chunk_summaries:
        logger.error("âŒ ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return None
    
    # è¦ç´„ã‚’çµåˆ
    combined_summary = "\n\n".join(chunk_summaries)
    
    # äºŒæ®µéšè¦ç´„ãŒæœ‰åŠ¹ãªå ´åˆã¯å…¨ä½“è¦ç´„ã‚’ç”Ÿæˆ
    if processing_config.get("two_stage_summary", False) and len(chunks) > 1:
        logger.info("å…¨ä½“è¦ç´„ã‚’ç”Ÿæˆã—ã¾ã™...")
        
        # å„ãƒãƒ£ãƒ³ã‚¯ã®è¦ç´„ã‚’ã¾ã¨ã‚ãŸå…¨ä½“è¦ç´„ã‚’ç”Ÿæˆ
        overall_summary = create_overall_summary(chunk_summaries, config)
        if overall_summary:
            combined_summary = f"# ä¼šè­°å…¨ä½“ã®è¦ç´„\n\n{overall_summary}\n\n# ãƒãƒ£ãƒ³ã‚¯åˆ¥è©³ç´°\n\n{combined_summary}"
            logger.info("å…¨ä½“è¦ç´„ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        else:
            logger.warning("å…¨ä½“è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    return combined_summary

def create_overall_summary(chunk_summaries: List[str], config: Dict[str, Any]) -> Optional[str]:
    """å„ãƒãƒ£ãƒ³ã‚¯ã®è¦ç´„ã‹ã‚‰å…¨ä½“è¦ç´„ã‚’ç”Ÿæˆã™ã‚‹
    
    Args:
        chunk_summaries: å„ãƒãƒ£ãƒ³ã‚¯ã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        config: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
        
    Returns:
        å…¨ä½“è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã€ã¾ãŸã¯å¤±æ•—æ™‚ã¯None
    """
    try:
        llm_config = config["llm"]
        api_type = llm_config["api_type"]
        
        # ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯è¦ç´„ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ†ã‚­ã‚¹ãƒˆ
        combined_text = "\n\n".join(chunk_summaries)
        
        # å…¨ä½“è¦ç´„ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""
ä»¥ä¸‹ã¯ä¼šè­°ã®å„ãƒ‘ãƒ¼ãƒˆã®è¦ç´„ã§ã™ã€‚ã“ã‚Œã‚‰ã®è¦ç´„ã‚’çµ±åˆã—ã¦ã€ä¼šè­°å…¨ä½“ã®ç°¡æ½”ãªè¦ç´„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

é‡è¦ãªç‚¹ï¼š
- å…¨ä½“ã®æµã‚Œã‚’æŠŠæ¡ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹
- é‡è¦ãªæ„æ€æ±ºå®šã‚„çµè«–ã‚’å¼·èª¿ã™ã‚‹
- çŸ›ç›¾ã™ã‚‹æƒ…å ±ãŒã‚ã‚Œã°èª¿æ•´ã—ã¦ä¸€è²«æ€§ã®ã‚ã‚‹è¦ç´„ã«ã™ã‚‹
- é‡è¤‡å†…å®¹ã¯ä¸€åº¦ã ã‘è¨˜è¼‰ã™ã‚‹

ä»¥ä¸‹ã®ä¼šè­°ãƒ‘ãƒ¼ãƒˆè¦ç´„ã‹ã‚‰å…¨ä½“è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

{combined_text}
"""
        
        logger.info(f"å…¨ä½“è¦ç´„ã®LLM API ({api_type}) å‘¼ã³å‡ºã—é–‹å§‹")
        
        # LLM APIå‘¼ã³å‡ºã—
        result = None
        if api_type == "openai":
            result = call_openai_api(prompt, llm_config)
        elif api_type == "anthropic":
            result = call_anthropic_api(prompt, llm_config)
        elif api_type == "google":
            result = call_google_api(prompt, llm_config)
        else:
            logger.error(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„APIç¨®é¡: {api_type}")
            return None
        
        if result:
            logger.info(f"âœ… å…¨ä½“è¦ç´„ã®ç”Ÿæˆå®Œäº†: ç´„{len(result)}æ–‡å­—ã®å¿œç­”ã‚’å—ä¿¡")
            return result
        else:
            logger.error("âŒ å…¨ä½“è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return None
    
    except Exception as e:
        logger.error(f"âŒ å…¨ä½“è¦ç´„ã®ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def call_openai_api(prompt: str, config: Dict[str, Any]) -> Optional[str]:
    """OpenAI APIã‚’å‘¼ã³å‡ºã™"""
    api_key = config["api_key"]
    if not api_key:
        logger.error("âŒ OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return None
    
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        data = {
            "model": config["model"],
            "messages": [
                {"role": "system", "content": "ã‚ãªãŸã¯ä¼šè­°ã®éŸ³å£°æ–‡å­—èµ·ã“ã—ã‹ã‚‰è­°äº‹éŒ²ã‚’ä½œæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": config["temperature"],
            "max_tokens": config["max_tokens"]
        }
        
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers=headers,
            json=data
        )
        
        if response.status_code == 200:
            result = response.json()
            return result["choices"][0]["message"]["content"]
        else:
            logger.error(f"âŒ APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}")
            return None
    
    except Exception as e:
        logger.error(f"âŒ OpenAI APIå‘¼ã³å‡ºã—ä¾‹å¤–: {e}")
        return None


def call_anthropic_api(prompt: str, config: Dict[str, Any]) -> Optional[str]:
    """Anthropic Claude APIã‚’å‘¼ã³å‡ºã™"""
    api_key = config["api_key"]
    if not api_key:
        logger.error("âŒ Anthropic APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return None
    
    try:
        # Claude Messages APIå½¢å¼ã§å‘¼ã³å‡ºã—
        headers = {
            "Content-Type": "application/json",
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01"  # APIãƒãƒ¼ã‚¸ãƒ§ãƒ³
        }
        
        data = {
            "model": config["model"],
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": config["temperature"],
            "max_tokens": config["max_tokens"]
        }
        
        response = requests.post(
            "https://api.anthropic.com/v1/messages",
            headers=headers,
            json=data
        )
        
        if response.status_code == 200:
            result = response.json()
            return result["content"][0]["text"]
        else:
            logger.error(f"âŒ APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}")
            return None
    
    except Exception as e:
        logger.error(f"âŒ Anthropic APIå‘¼ã³å‡ºã—ä¾‹å¤–: {e}")
        return None


def call_google_api(prompt: str, config: Dict[str, Any]) -> Optional[str]:
    """Google Gemini APIã‚’å‘¼ã³å‡ºã™"""
    api_key = config.get("google_api_key", "")
    if not api_key:
        logger.error("âŒ Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return None
    
    try:
        # Gemini API URL
        model = config["model"]
        # ãƒ¢ãƒ‡ãƒ«åã«åŸºã¥ã„ã¦APIãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        # APIä»•æ§˜ã«åˆã‚ã›ã¦ãƒ¢ãƒ‡ãƒ«åã‚’ãã®ã¾ã¾ä½¿ç”¨
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
        
        headers = {
            "Content-Type": "application/json"
        }
        
        data = {
            "contents": [
                {
                    "role": "user",
                    "parts": [{"text": prompt}]
                }
            ],
            "generationConfig": {
                "temperature": config["temperature"],
                "maxOutputTokens": config["max_tokens"],
                "topP": 0.95,
                "topK": 40
            }
        }
        
        response = requests.post(url, headers=headers, json=data)
        
        if response.status_code == 200:
            result = response.json()
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã«åˆã‚ã›ã¦é©åˆ‡ã«ãƒ‘ãƒ¼ã‚¹ã™ã‚‹
            if "candidates" in result and len(result["candidates"]) > 0:
                if "content" in result["candidates"][0]:
                    if "parts" in result["candidates"][0]["content"]:
                        text_parts = []
                        for part in result["candidates"][0]["content"]["parts"]:
                            if "text" in part:
                                text_parts.append(part["text"])
                        return "".join(text_parts)
            
            logger.error(f"Google APIå¿œç­”ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {result}")
            return None
        else:
            logger.error(f"âŒ Google APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {response.status_code} - {response.text}")
            return None
    
    except Exception as e:
        logger.error(f"âŒ Google APIå‘¼ã³å‡ºã—ä¾‹å¤–: {e}")
        return None


def save_output(content: str, original_file: str, config: Dict[str, Any]) -> str:
    """ç”Ÿæˆã•ã‚ŒãŸè­°äº‹éŒ²ã‚’ä¿å­˜"""
    output_dir = config["file_watcher"]["output_directory"]
    if not output_dir:
        output_dir = os.path.dirname(original_file)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    base_name = os.path.splitext(os.path.basename(original_file))[0]
    timestamp = datetime.now().strftime("%Y-%m%d-%H%M")
    output_file = os.path.join(output_dir, f"{base_name}_memo_{timestamp}.txt")
    
    # å†…å®¹ã‚’ä¿å­˜
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(content)
    
    logger.info(f"è­°äº‹éŒ²ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
    return output_file


def process_file_queue():
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ¥ãƒ¼ã‚’å‡¦ç†"""
    global should_stop, config
    
    logger.info("ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
    config = load_config()
    
    while not should_stop:
        try:
            # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
            try:
                file_path = file_queue.get(timeout=1)
            except queue.Empty:
                continue
            
            # å‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹ã®å†ãƒã‚§ãƒƒã‚¯ï¼ˆã‚­ãƒ¥ãƒ¼ã«å…¥ã£ãŸå¾Œã«ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ã§å‡¦ç†ã•ã‚ŒãŸå¯èƒ½æ€§ï¼‰
            if is_file_processed(file_path):
                logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™ï¼ˆã‚­ãƒ¥ãƒ¼å†…å†ãƒã‚§ãƒƒã‚¯ï¼‰: {file_path}")
                file_queue.task_done()
                continue
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            base_filename = os.path.basename(file_path)
            logger.info(f"ğŸ”„ ===== å‡¦ç†é–‹å§‹: {base_filename} =====")
            logger.info(f"ğŸ“‹ å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ— [1/4]: æ–‡å­—èµ·ã“ã—æº–å‚™")
            
            # 1. æ–‡å­—èµ·ã“ã—
            logger.info(f"ğŸ”„ å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ— [2/4]: æ–‡å­—èµ·ã“ã—å®Ÿè¡Œä¸­...")
            transcription = transcribe_file(file_path, config)
            if not transcription or should_stop:
                logger.warning(f"âŒ æ–‡å­—èµ·ã“ã—ã«å¤±æ•—ã¾ãŸã¯ä¸­æ–­ã•ã‚Œã¾ã—ãŸ: {file_path}")
                file_queue.task_done()
                continue
            
            # æ–‡å­—èµ·ã“ã—çµæœã‚’ä¿å­˜
            base_name = os.path.splitext(os.path.basename(file_path))[0]
            timestamp = datetime.now().strftime("%Y-%m%d-%H%M")
            
            # æ–‡å­—èµ·ã“ã—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
            transcript_dir = config["file_watcher"]["transcript_directory"]
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã¯ validate_config ã§è¡Œã‚ã‚Œã¦ã„ã‚‹ã®ã§ã€ã“ã“ã§ã¯çœç•¥
            
            transcript_file = os.path.join(transcript_dir, f"{base_name}_transcript_{timestamp}.txt")
            with open(transcript_file, "w", encoding="utf-8") as f:
                f.write(transcription)
            logger.info(f"âœ… æ–‡å­—èµ·ã“ã—çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {transcript_file}")
            
            # 2. LLM APIå‘¼ã³å‡ºã—
            logger.info(f"ğŸ”„ å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ— [3/4]: LLM APIã§è­°äº‹éŒ²ç”Ÿæˆä¸­...")
            memo = call_llm_api(transcription, config)
            
            if not memo or should_stop:
                logger.warning(f"âŒ è­°äº‹éŒ²ç”Ÿæˆã«å¤±æ•—ã¾ãŸã¯ä¸­æ–­ã•ã‚Œã¾ã—ãŸ: {file_path}")
                file_queue.task_done()
                continue
            
            # 3. çµæœã‚’ä¿å­˜
            logger.info(f"ğŸ”„ å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ— [4/4]: è­°äº‹éŒ²ä¿å­˜ä¸­...")
            output_file = save_output(memo, file_path, config)
            
            logger.info(f"âœ…âœ… å‡¦ç†å®Œäº†: {base_filename}")
            logger.info(f"ğŸ“„ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
            logger.info(f"===== å‡¦ç†çµ‚äº†: {base_filename} =====")
            
            # 4. å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
            mark_file_as_processed(file_path, output_file)
            
            # ã‚­ãƒ¥ãƒ¼ã®ã‚¿ã‚¹ã‚¯å®Œäº†ã‚’é€šçŸ¥
            file_queue.task_done()
        
        except Exception as e:
            logger.exception(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            try:
                file_queue.task_done()
            except:
                pass
    
    logger.info("ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚")


def start_file_watcher(config: Dict[str, Any]) -> Optional[Observer]:
    """ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’é–‹å§‹"""
    input_dir = config["file_watcher"]["input_directory"]
    
    if not input_dir:
        logger.error("å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return None
    
    if not os.path.exists(input_dir):
        try:
            os.makedirs(input_dir, exist_ok=True)
            logger.info(f"å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {input_dir}")
        except Exception as e:
            logger.error(f"å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None
    
    try:
        event_handler = MediaFileHandler(config)
        observer = Observer()
        observer.schedule(event_handler, input_dir, recursive=False)
        observer.start()
        logger.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›£è¦–ã‚’é–‹å§‹ã—ã¾ã—ãŸ: {input_dir}")
        return observer
    
    except Exception as e:
        logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None


def check_existing_files(config: Dict[str, Any]):
    """ç›£è¦–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ """
    input_dir = config["file_watcher"]["input_directory"]
    if not input_dir or not os.path.exists(input_dir):
        return
    
    extensions = config["file_watcher"]["supported_extensions"]
    count = 0
    processed_count = 0
    
    for file in os.listdir(input_dir):
        file_path = os.path.join(input_dir, file)
        if os.path.isfile(file_path):
            ext = os.path.splitext(file)[-1].lower()
            if ext in extensions:
                # å‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹ãƒã‚§ãƒƒã‚¯
                if is_file_processed(file_path):
                    processed_count += 1
                    continue
                    
                file_queue.put(file_path)
                count += 1
    
    if count > 0:
        logger.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®æœªå‡¦ç†ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ« {count}å€‹ ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã—ã¾ã—ãŸã€‚")
    if processed_count > 0:
        logger.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å‡¦ç†æ¸ˆã¿ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ•ã‚¡ã‚¤ãƒ« {processed_count}å€‹ ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")


def validate_config(config: Dict[str, Any]) -> bool:
    """è¨­å®šã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
    # å¿…é ˆã‚­ãƒ¼ã®ç¢ºèª
    required_keys = [
        "transcription", "llm", "file_watcher", "prompt_templates"
    ]
    
    for key in required_keys:
        if key not in config:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…é ˆã‚­ãƒ¼ '{key}' ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return False
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®šã‚’ç¢ºèª
    if not config["file_watcher"]["input_directory"]:
        logger.error("å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šGUIã‹ã‚‰è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return False
    
    if not config["file_watcher"]["output_directory"]:
        logger.error("å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šGUIã‹ã‚‰è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return False
    
    # æ–‡å­—èµ·ã“ã—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    transcript_dir = config["file_watcher"].get("transcript_directory", "")
    if not transcript_dir:
        logger.error("æ–‡å­—èµ·ã“ã—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šGUIã‹ã‚‰è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return False
        
    # å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã¨ä½œæˆå‡¦ç†
    input_dir = config["file_watcher"]["input_directory"]
    output_dir = config["file_watcher"]["output_directory"]
    
    for dir_name, dir_path in [("å…¥åŠ›", input_dir), ("æ–‡å­—èµ·ã“ã—", transcript_dir), ("å‡ºåŠ›", output_dir)]:
        if not os.path.exists(dir_path):
            try:
                os.makedirs(dir_path, exist_ok=True)
                logger.info(f"{dir_name}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {dir_path}")
            except Exception as e:
                logger.error(f"{dir_name}ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return False
    
    # LLM APIè¨­å®šã®ç¢ºèª
    api_type = config["llm"]["api_type"]
    if api_type == "openai" and not config["llm"]["api_key"]:
        logger.warning("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šGUIã‹ã‚‰è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    elif api_type == "anthropic" and not config["llm"]["api_key"]:
        logger.warning("Anthropic APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šGUIã‹ã‚‰è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    elif api_type == "google" and not config["llm"].get("google_api_key"):
        logger.warning("Google APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šGUIã‹ã‚‰è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    
    # å…¥å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒã‚§ãƒƒã‚¯ï¼ˆç•°ãªã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¨å¥¨ï¼‰
    if input_dir == output_dir:
        logger.warning("å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒåŒã˜ã§ã™ã€‚åˆ¥ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
    
    return True


def start_service():
    """ã‚µãƒ¼ãƒ“ã‚¹ã®é–‹å§‹"""
    global processing_thread, observer, should_stop, config
    
    # è¨­å®šã®èª­ã¿è¾¼ã¿ã¨æ¤œè¨¼
    config = load_config()
    if not validate_config(config):
        logger.error("è¨­å®šã®æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return False
    
    # FFmpegã®ãƒã‚§ãƒƒã‚¯
    if not check_ffmpeg():
        logger.error("FFmpegãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ã‹ã€PATHã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        logger.error("FFmpegã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã€PATHã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
        return False
    
    # å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    clean_old_processed_files()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã®é–‹å§‹
    should_stop = False
    processing_thread = threading.Thread(target=process_file_queue, daemon=True)
    processing_thread.start()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã®é–‹å§‹
    observer = start_file_watcher(config)
    if not observer:
        stop_service()
        return False
    
    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯
    check_existing_files(config)
    
    logger.info("ğŸš€ KoeMemoã‚µãƒ¼ãƒ“ã‚¹ãŒé–‹å§‹ã•ã‚Œã¾ã—ãŸã€‚")
    return True


def stop_service():
    """ã‚µãƒ¼ãƒ“ã‚¹ã®åœæ­¢"""
    global processing_thread, observer, should_stop
    
    # åœæ­¢ãƒ•ãƒ©ã‚°ã®è¨­å®š
    should_stop = True
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã®åœæ­¢
    if observer:
        observer.stop()
        observer.join()
        observer = None
    
    # å‡¦ç†ã‚¹ãƒ¬ãƒƒãƒ‰ã®å¾…æ©Ÿ
    if processing_thread and processing_thread.is_alive():
        processing_thread.join(timeout=5)
        processing_thread = None
    
    logger.info("ğŸ›‘ KoeMemoã‚µãƒ¼ãƒ“ã‚¹ãŒåœæ­¢ã•ã‚Œã¾ã—ãŸã€‚")


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    print("KoeMemo - éŸ³å£°ãƒ»å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•çš„ã«è­°äº‹éŒ²ã‚’ç”Ÿæˆã™ã‚‹ãƒ„ãƒ¼ãƒ«")
    print("======================================================")
    
    try:
        print("\nKoeMemoã‚µãƒ¼ãƒ“ã‚¹ã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...")
        if start_service():
            print("ã‚µãƒ¼ãƒ“ã‚¹ãŒæ­£å¸¸ã«é–‹å§‹ã•ã‚Œã¾ã—ãŸã€‚Ctrl+Cã§çµ‚äº†ã§ãã¾ã™ã€‚")
            
            # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ç¶­æŒ
            try:
                while True:
                    time.sleep(1)
            except KeyboardInterrupt:
                print("\nKoeMemoã‚µãƒ¼ãƒ“ã‚¹ã‚’åœæ­¢ã—ã¦ã„ã¾ã™...")
                stop_service()
                print("ã‚µãƒ¼ãƒ“ã‚¹ãŒåœæ­¢ã—ã¾ã—ãŸã€‚")
        else:
            print("ã‚µãƒ¼ãƒ“ã‚¹ã®é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    
    except Exception as e:
        logger.exception(f"äºˆæœŸã—ãªã„ä¾‹å¤–ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        stop_service()
        return 1
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
